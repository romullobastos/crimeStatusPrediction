import streamlit as st
import streamlit.components.v1 as components
import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, make_scorer, roc_auc_score, f1_score
from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
import folium
import warnings
warnings.filterwarnings('ignore')

# Configura√ß√£o do Plotly para suprimir avisos de deprecia√ß√£o
import plotly.io as pio
pio.templates.default = "plotly"

# Configura√ß√£o para suprimir avisos espec√≠ficos do Plotly
import logging
logging.getLogger('plotly').setLevel(logging.CRITICAL)
logging.getLogger('plotly.graph_objects').setLevel(logging.CRITICAL)
logging.getLogger('plotly.express').setLevel(logging.CRITICAL)

# Configura√ß√£o adicional para suprimir avisos de deprecia√ß√£o
import os
os.environ['PLOTLY_DISABLE_DEPRECATION_WARNINGS'] = '1'

# Suprimir warnings do Python relacionados ao Plotly
import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning, module='plotly')
warnings.filterwarnings('ignore', message='.*keyword arguments have been deprecated.*')
warnings.filterwarnings('ignore', message='.*deprecated.*')
warnings.filterwarnings('ignore', message='.*will be removed.*')
warnings.filterwarnings('ignore', message='.*Use config instead.*')
warnings.filterwarnings('ignore', message='.*Please replace.*')

# Configura√ß√£o adicional para suprimir avisos do Plotly
import logging
logging.getLogger('plotly').disabled = True
logging.getLogger('plotly.graph_objects').disabled = True
logging.getLogger('plotly.express').disabled = True

# Importar bibliotecas necess√°rias
import plotly.express as px
import plotly.graph_objects as go

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Predi√ß√£o de Status de Crimes",
    page_icon="üîç",
    layout="wide"
)

# T√≠tulo principal
st.title("üîç Sistema de Predi√ß√£o de Crimes")
st.markdown("**Sistema Inteligente para Prever se um Crime ser√° Resolvido ou Arquivado**")
st.markdown("*Baseado em caracter√≠sticas como: tipo de crime, como foi cometido, arma usada e n√∫mero de pessoas envolvidas*")

# Organiza√ß√£o visual
st.markdown("---")

# Se√ß√£o principal com layout melhorado
col1, col2 = st.columns([2, 1])

with col1:
    st.header("ü§ñ Como Funciona o Sistema")
    st.markdown("""
    **O sistema usa intelig√™ncia artificial para analisar crimes e prever se eles ser√£o resolvidos ou arquivados.**
    """)


# Cards informativos
col1, col2 = st.columns(2)

with col1:
    st.markdown("""
    <div style="background-color: #e8f4fd; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
    <h4 style="margin-top: 0; color: #1f77b4;">üîç O que o sistema analisa</h4>
    <ul style="margin-bottom: 0;">
    <li>Tipo de crime cometido</li>
    <li>Como o crime foi executado</li>
    <li>Arma utilizada</li>
    <li>Quantidade de v√≠timas</li>
    <li>Quantidade de suspeitos</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)

with col2:
    st.markdown("""
    <div style="background-color: #f0f8e8; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
    <h4 style="margin-top: 0; color: #2e7d32;">üìä O que o sistema faz</h4>
    <ul style="margin-bottom: 0;">
    <li>Agrupa crimes similares</li>
    <li>Identifica padr√µes de resolu√ß√£o</li>
    <li>Detecta casos at√≠picos</li>
    <li>Calcula probabilidades</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)

# Carregar dados
@st.cache_data(show_spinner=False)
def load_data():
    df = pd.read_csv('dataset_ocorrencias_delegacia_5.csv')
    
    # Converter data_ocorrencia para datetime
    df['data_ocorrencia'] = pd.to_datetime(df['data_ocorrencia'])
    
    return df

# Carregar dataset padr√£o
df = load_data()
st.sidebar.info("üìÅ Usando dataset padr√£o")

# Preparar dados para o modelo (REGRESS√ÉO LOG√çSTICA)
@st.cache_data(show_spinner=False)
def prepare_data(df):
    # Selecionar features categ√≥ricas e num√©ricas (alinhadas com clustering)
    categorical_features = ['tipo_crime', 'descricao_modus_operandi', 'arma_utilizada']
    numerical_features = ['quantidade_vitimas', 'quantidade_suspeitos']
    
    # Codificar vari√°veis categ√≥ricas
    le_dict = {}
    df_encoded = df.copy()
    
    for feature in categorical_features:
        le = LabelEncoder()
        df_encoded[feature + '_encoded'] = le.fit_transform(df_encoded[feature].astype(str))
        le_dict[feature] = le
    
    # Selecionar features para o modelo
    feature_columns = [f + '_encoded' for f in categorical_features] + numerical_features
    X = df_encoded[feature_columns]
    y = df_encoded['status_binario']
    
    return X, y, le_dict, feature_columns

# Filtrar dados (excluir "Em Investiga√ß√£o")
df_filtered = df[~df['status_investigacao'].str.contains('Em Investiga', na=False)].copy()
df_filtered['status_binario'] = (df_filtered['status_investigacao'].str.contains('Conclu', na=False)).astype(int)

# Mostrar informa√ß√µes sobre o dataset
st.sidebar.info(f"üìä Dataset: {len(df)} registros")
st.sidebar.info(f"üìã Colunas: {len(df.columns)}")


# Preparar dados
X, y, le_dict, feature_columns = prepare_data(df_filtered)

# Dividir dados
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Normalizar dados
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Criar modelo de clustering
@st.cache_resource(show_spinner=False)
def create_clustering_model(df):
    """Cria modelo de clustering com as mesmas features da regress√£o"""
    categorical_features = ['tipo_crime', 'descricao_modus_operandi', 'arma_utilizada']
    numerical_features = ['quantidade_vitimas', 'quantidade_suspeitos']
    
    df_cluster = df.copy()
    le_cluster = {}
    
    for feature in categorical_features:
        le = LabelEncoder()
        df_cluster[feature + '_encoded'] = le.fit_transform(df_cluster[feature].astype(str))
        le_cluster[feature] = le
    
    cluster_columns = [f + '_encoded' for f in categorical_features] + numerical_features
    X_cluster = df_cluster[cluster_columns]
    
    scaler_cluster = StandardScaler()
    X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)
    
    kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X_cluster_scaled)
    
    df_cluster['cluster'] = clusters
    
    return df_cluster, kmeans, scaler_cluster, le_cluster, cluster_columns

# Fun√ß√£o para detec√ß√£o de anomalias
@st.cache_resource(show_spinner=False)
def detect_anomalies(df):
    """Detecta anomalias usando Isolation Forest e LOF"""
    categorical_features = ['tipo_crime', 'descricao_modus_operandi', 'arma_utilizada']
    numerical_features = ['quantidade_vitimas', 'quantidade_suspeitos']
    
    df_anomaly = df.copy()
    le_anomaly = {}
    
    for feature in categorical_features:
        le = LabelEncoder()
        df_anomaly[feature + '_encoded'] = le.fit_transform(df_anomaly[feature].astype(str))
        le_anomaly[feature] = le
    
    anomaly_columns = [f + '_encoded' for f in categorical_features] + numerical_features
    X_anomaly = df_anomaly[anomaly_columns]
    
    scaler_anomaly = StandardScaler()
    X_anomaly_scaled = scaler_anomaly.fit_transform(X_anomaly)
    
    iso_forest = IsolationForest(contamination=0.1, random_state=42)
    iso_anomalies = iso_forest.fit_predict(X_anomaly_scaled)
    
    # Ajustar n_neighbors baseado no tamanho dos dados
    n_neighbors = min(20, max(5, len(X_anomaly_scaled) // 10))
    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=0.1, novelty=True)
    lof.fit(X_anomaly_scaled)
    lof_anomalies = lof.predict(X_anomaly_scaled)
    
    df_anomaly['iso_anomaly'] = iso_anomalies
    df_anomaly['lof_anomaly'] = lof_anomalies
    df_anomaly['is_anomaly'] = ((iso_anomalies == -1) | (lof_anomalies == -1)).astype(int)
    
    return df_anomaly, iso_forest, lof, scaler_anomaly, le_anomaly, anomaly_columns

# Criar modelo de clustering
df_with_clusters, kmeans_model, scaler_cluster, le_cluster, cluster_columns = create_clustering_model(df_filtered)

# Detectar anomalias
df_with_anomalies, iso_model, lof_model, scaler_anomaly, le_anomaly, anomaly_columns = detect_anomalies(df_filtered)

# Treinar modelo
st.header("üéØ Fa√ßa sua Predi√ß√£o")

st.markdown("""
**Selecione as caracter√≠sticas do crime abaixo e o sistema ir√°:**
- Calcular a probabilidade de ser resolvido ou arquivado
- Mostrar em qual grupo de crimes similares ele se encaixa
- Indicar se √© um caso at√≠pico que merece aten√ß√£o especial
""")

model_choice = st.selectbox("Escolha o modelo:", ["Random Forest"], disabled=True)
st.info("üí° **Random Forest** √© um algoritmo de intelig√™ncia artificial que combina m√∫ltiplas '√°rvores de decis√£o' para fazer predi√ß√µes mais precisas e confi√°veis.")

# Configura√ß√µes do sistema
st.subheader("‚öôÔ∏è Configura√ß√µes do Sistema")

# Explica√ß√£o simples sobre tunagem
st.markdown("""
**O que √© ajuste autom√°tico?**
- O sistema pode testar diferentes configura√ß√µes para encontrar a melhor precis√£o
- Isso pode melhorar a qualidade das predi√ß√µes, mas demora mais tempo
- Voc√™ pode escolher se quer usar ou n√£o
""")

# Op√ß√£o simples de ativar/desativar
tuning_enabled = st.radio(
    "Escolha uma op√ß√£o:",
    ["üöÄ Usar configura√ß√µes r√°pidas (recomendado)", "üîç Ajustar automaticamente para melhor precis√£o"],
    help="A primeira op√ß√£o √© mais r√°pida, a segunda pode ser mais precisa"
)

# Se escolher ajuste autom√°tico, mostrar op√ß√µes simples
if "Ajustar automaticamente" in tuning_enabled:
    st.markdown("**Configura√ß√µes de ajuste:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Op√ß√£o simples de velocidade vs precis√£o
        speed_choice = st.selectbox(
            "Velocidade do ajuste:",
            ["R√°pido (5 tentativas)", "M√©dio (15 tentativas)", "Lento (30 tentativas)"],
            help="Mais tentativas = melhor precis√£o, mas demora mais"
        )
        
        # Mapear para n√∫mero de itera√ß√µes
        if "R√°pido" in speed_choice:
            n_iter = 5
        elif "M√©dio" in speed_choice:
            n_iter = 15
        else:
            n_iter = 30
    
    with col2:
        # Crit√©rio de qualidade simplificado
        quality_choice = st.selectbox(
            "Crit√©rio de qualidade:",
            ["Precis√£o geral", "Balanceamento de classes"],
            help="Precis√£o geral: foca na acur√°cia total. Balanceamento: trata classes desiguais melhor"
        )
        
        # Mapear para scoring
        if "Precis√£o geral" in quality_choice:
            scoring = 'f1'
        else:
            scoring = 'roc_auc'
    
    # Configura√ß√µes fixas para simplificar
    search_type = "Busca Aleat√≥ria"
    cv_folds = 5
    
    st.info(f"üîß O sistema vai testar {n_iter} configura√ß√µes diferentes para encontrar a melhor precis√£o.")
else:
    # Configura√ß√µes padr√£o quando n√£o usar tunagem
    search_type = "Busca Exaustiva"
    scoring = 'roc_auc'
    cv_folds = 5
    n_iter = None

# Preparar objetos de tunagem
best_params = None
best_cv_score = None

# Fun√ß√£o de treinamento com cache inteligente
@st.cache_resource(show_spinner=False)
def train_model_with_cache(X_train, y_train, tuning_enabled, search_type, scoring, cv_folds, n_iter):
    """Treina modelo com cache baseado nas configura√ß√µes de tunagem"""
    
    if "Ajustar automaticamente" in tuning_enabled:
        # Espa√ßo de busca otimizado para Random Forest
        param_grid_rf = {
            'n_estimators': [100, 200, 300],
            'max_depth': [None, 10, 20],
            'max_features': ['sqrt', 'log2'],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2],
            'bootstrap': [True, False],
            'class_weight': [None, 'balanced']
        }
        
        if search_type == "Busca Aleat√≥ria":
            search = RandomizedSearchCV(
                RandomForestClassifier(random_state=42),
                param_grid_rf,
                n_iter=n_iter,
                cv=cv_folds,
                scoring=scoring,
                random_state=42,
                n_jobs=-1
            )
        else:
            search = GridSearchCV(
                RandomForestClassifier(random_state=42),
                param_grid_rf,
                cv=cv_folds,
                scoring=scoring,
                n_jobs=-1
            )
        
        search.fit(X_train, y_train)
        return search.best_estimator_, search.best_params_, search.best_score_
    else:
        # Usar configura√ß√µes otimizadas padr√£o
        model = RandomForestClassifier(
            n_estimators=200,
            max_depth=None,
            max_features='sqrt',
            min_samples_split=10,
            min_samples_leaf=1,
            bootstrap=False,
            class_weight='balanced',
            random_state=42
        )
        model.fit(X_train, y_train)
        return model, None, None

# Treinar modelo
with st.spinner("üîç Treinando modelo..."):
    model, best_params, best_cv_score = train_model_with_cache(
        X_train, y_train, tuning_enabled, search_type, scoring, cv_folds, n_iter
    )

# Predi√ß√µes
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Interface de predi√ß√£o
st.header("üìù Preencha as Informa√ß√µes do Crime")

col1, col2 = st.columns(2)

with col1:
    st.subheader("Caracter√≠sticas do Crime:")
    
    # Inputs para predi√ß√£o (apenas features relevantes)
    tipo_crime = st.selectbox("Tipo de Crime", df['tipo_crime'].unique())
    modus_operandi = st.selectbox("Como foi cometido", df['descricao_modus_operandi'].unique())
    arma = st.selectbox("Arma Utilizada", df['arma_utilizada'].unique())

with col2:
    st.subheader("Pessoas Envolvidas:")
    
    qtd_vitimas = st.slider("Quantidade de V√≠timas", 0, 4, 1)
    qtd_suspeitos = st.slider("Quantidade de Suspeitos", 0, 4, 1)

# Bot√£o de predi√ß√£o
if st.button("üîÆ Analisar Crime", type="primary"):
    # Preparar dados de entrada
    input_data = {
        'tipo_crime': tipo_crime,
        'descricao_modus_operandi': modus_operandi,
        'arma_utilizada': arma,
        'quantidade_vitimas': qtd_vitimas,
        'quantidade_suspeitos': qtd_suspeitos
    }
    
    # Converter para DataFrame
    input_df = pd.DataFrame([input_data])
    
    # Codificar vari√°veis categ√≥ricas para regress√£o
    for feature in ['tipo_crime', 'descricao_modus_operandi', 'arma_utilizada']:
        input_df[feature + '_encoded'] = le_dict[feature].transform(input_df[feature].astype(str))
    
    # Selecionar features para regress√£o
    X_input = input_df[feature_columns]
    
    # Fazer predi√ß√£o de status
    proba = model.predict_proba(X_input)[0]
    
    # Usar os mesmos dados para clustering (features j√° alinhadas)
    input_df_cluster = input_df.copy()
    
    # Codificar vari√°veis categ√≥ricas para clustering
    for feature in ['tipo_crime', 'descricao_modus_operandi', 'arma_utilizada']:
        input_df_cluster[feature + '_encoded'] = le_cluster[feature].transform(input_df_cluster[feature].astype(str))
    
    # Selecionar features para clustering
    X_input_cluster = input_df_cluster[cluster_columns]
    X_input_cluster_scaled = scaler_cluster.transform(X_input_cluster)
    
    # Fazer predi√ß√£o de cluster
    predicted_cluster = kmeans_model.predict(X_input_cluster_scaled)[0]
    
    # Verificar se √© anomalia
    X_input_anomaly = input_df_cluster[anomaly_columns]
    X_input_anomaly_scaled = scaler_anomaly.transform(X_input_anomaly)

    iso_pred = iso_model.predict(X_input_anomaly_scaled)[0]
    
    # Usar o modelo LOF j√° treinado para predi√ß√£o
    lof_pred = lof_model.predict(X_input_anomaly_scaled)[0]
    is_anomaly = (iso_pred == -1) or (lof_pred == -1)
    
    # Exibir resultados
    st.subheader("üéØ O Que o Sistema Descobriu")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Probabilidade de Arquivamento", f"{proba[0]:.1%}")
        st.metric("Probabilidade de Conclus√£o", f"{proba[1]:.1%}")
    
    with col2:
        st.metric("Grupo de Crimes Similares", f"Grupo {predicted_cluster}")
        
        # An√°lise do cluster predito
        cluster_data = df_with_clusters[df_with_clusters['cluster'] == predicted_cluster]
        cluster_completion_rate = cluster_data['status_binario'].mean() * 100
        st.metric("Taxa de Conclus√£o do Grupo", f"{cluster_completion_rate:.1f}%")
    
    with col3:
        # Status da anomalia
        if is_anomaly:
            st.error("üö® **CASO AT√çPICO**")
            st.markdown("Este crime tem caracter√≠sticas muito diferentes dos casos normais e merece aten√ß√£o especial.")
        else:
            st.success("‚úÖ **CASO PADR√ÉO**")
            st.markdown("Este crime segue padr√µes similares aos casos j√° conhecidos.")

        # Exibir probabilidades em formato de texto
        st.info(f"üìä **Probabilidade de Arquivamento:** {proba[0]:.1%} | **Probabilidade de Conclus√£o:** {proba[1]:.1%}")
    
    # An√°lise do cluster predito
    st.subheader(f"üìä O Que Sabemos Sobre o Grupo {predicted_cluster}")
    
    cluster_analysis = cluster_data.groupby('status_investigacao').size()
    cluster_analysis_pct = cluster_data['status_investigacao'].value_counts(normalize=True) * 100
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**üìä Como os crimes deste grupo costumam terminar:**")
        st.dataframe(cluster_analysis_pct.round(1))
    
    with col2:
        # Caracter√≠sticas dominantes do cluster
        st.write("**üîç O que √© comum neste grupo de crimes:**")
        tipo_dominante = cluster_data['tipo_crime'].mode()[0]
        modus_dominante = cluster_data['descricao_modus_operandi'].mode()[0]
        arma_dominante = cluster_data['arma_utilizada'].mode()[0]
        
        st.write(f"‚Ä¢ **Tipo de Crime mais comum:** {tipo_dominante}")
        st.write(f"‚Ä¢ **Como costuma ser cometido:** {modus_dominante}")
        st.write(f"‚Ä¢ **Arma mais usada:** {arma_dominante}")
        st.write(f"‚Ä¢ **N√∫mero m√©dio de v√≠timas:** {cluster_data['quantidade_vitimas'].mean():.1f}")
        st.write(f"‚Ä¢ **Suspeitos m√©dios:** {cluster_data['quantidade_suspeitos'].mean():.1f}")
    
    # Interpreta√ß√£o
    if proba[1] > 0.6:
        st.success("‚úÖ **Alta probabilidade de CONCLUS√ÉO** - O caso tem caracter√≠sticas que favorecem a resolu√ß√£o da investiga√ß√£o.")
    elif proba[0] > 0.6:
        st.warning("‚ö†Ô∏è **Alta probabilidade de ARQUIVAMENTO** - O caso tem caracter√≠sticas que podem levar ao arquivamento.")
    else:
        st.info("ü§î **Probabilidades equilibradas** - O caso pode ter qualquer um dos dois desfechos.")
    
    # Interpreta√ß√£o do cluster
    if cluster_completion_rate > 60:
        st.info(f"üîç **Grupo {predicted_cluster}** tem alta taxa de conclus√£o ({cluster_completion_rate:.1f}%), indicando que crimes similares tendem a ser resolvidos.")
    elif cluster_completion_rate < 40:
        st.warning(f"üîç **Grupo {predicted_cluster}** tem baixa taxa de conclus√£o ({cluster_completion_rate:.1f}%), indicando que crimes similares tendem a ser arquivados.")
    else:
        st.info(f"üîç **Grupo {predicted_cluster}** tem taxa equilibrada de conclus√£o ({cluster_completion_rate:.1f}%).")

# Tela de Anomalias
st.markdown("---")
st.header("üö® Detec√ß√£o de Casos At√≠picos")

st.markdown("""
**O sistema identifica crimes que s√£o muito diferentes dos casos normais.**

**üîç Por que isso √© importante?**
- **Casos at√≠picos** podem indicar novos tipos de crime
- **Merecem aten√ß√£o especial** da pol√≠cia
- **Podem revelar padr√µes** que n√£o foram identificados antes
- **Ajudam a melhorar** as estrat√©gias de investiga√ß√£o

**üìä Como funciona:**
O sistema compara cada crime com todos os outros e identifica aqueles que t√™m caracter√≠sticas muito diferentes do padr√£o normal.
""")

# Estat√≠sticas de anomalias
anomaly_stats = df_with_anomalies['is_anomaly'].value_counts()
total_anomalies = anomaly_stats.get(1, 0)
total_normal = anomaly_stats.get(0, 0)

col1, col2, col3 = st.columns(3)
with col1:
    st.metric("Casos At√≠picos Encontrados", total_anomalies)
with col2:
    st.metric("Casos Normais", total_normal)
with col3:
    anomaly_rate = (total_anomalies / len(df_with_anomalies)) * 100
    st.metric("Taxa de Casos At√≠picos", f"{anomaly_rate:.1f}%")

# Filtros para anomalias
st.subheader("üîç Como Quer Ver os Casos At√≠picos?")

st.markdown("""
**O que s√£o esses filtros?**
- O sistema usa duas formas diferentes de encontrar casos at√≠picos
- Voc√™ pode escolher ver casos encontrados por cada m√©todo ou por ambos
- Isso ajuda a entender melhor quais casos s√£o realmente diferentes
""")

col1, col2 = st.columns(2)
with col1:
    show_iso_anomalies = st.checkbox("üìä Mostrar casos encontrados pelo m√©todo principal", value=True, help="M√©todo que identifica casos muito diferentes do padr√£o normal")
with col2:
    show_lof_anomalies = st.checkbox("üîç Mostrar casos encontrados pelo m√©todo de compara√ß√£o", value=True, help="M√©todo que compara com casos similares para encontrar diferen√ßas")

# Filtrar anomalias
anomaly_filter = df_with_anomalies['is_anomaly'] == 1
if show_iso_anomalies and not show_lof_anomalies:
    anomaly_filter = df_with_anomalies['iso_anomaly'] == -1
elif show_lof_anomalies and not show_iso_anomalies:
    anomaly_filter = df_with_anomalies['lof_anomaly'] == -1

anomalies_df = df_with_anomalies[anomaly_filter]

if len(anomalies_df) > 0:
    st.subheader(f"üìã Casos Que Precisam de Aten√ß√£o Especial ({len(anomalies_df)} casos encontrados)")
    
    st.info("""
    üí° **Por que estes casos s√£o especiais?**
    - S√£o muito diferentes dos crimes normais que vemos
    - Podem ter caracter√≠sticas √∫nicas que merecem investiga√ß√£o especial
    - Podem indicar novos tipos de crimes ou padr√µes criminais
    """)
    
    # Selecionar colunas para exibir
    display_columns = ['tipo_crime', 'descricao_modus_operandi', 'arma_utilizada', 
                      'quantidade_vitimas', 'quantidade_suspeitos', 'status_investigacao']
    
    # Filtrar apenas colunas que existem
    available_columns = [col for col in display_columns if col in anomalies_df.columns]
    
    if available_columns:
        # Renomear colunas para melhor compreens√£o
        display_df = anomalies_df[available_columns].copy()
        display_df.columns = ['Tipo de Crime', 'Como foi Cometido', 'Arma Utilizada', 
                             'Quantidade de V√≠timas', 'Quantidade de Suspeitos', 'Desfecho']
        
        st.dataframe(display_df, width='stretch')
        
        # An√°lise das anomalias
        st.subheader("üìä O Que Podemos Aprender Destes Casos?")
        
        # Cards de estat√≠sticas gerais
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("üîç Total de Casos At√≠picos", len(anomalies_df))
        
        with col2:
            arquivados = len(anomalies_df[anomalies_df['status_investigacao'].str.contains('Arquivado', na=False)])
            taxa_arquivamento = (arquivados / len(anomalies_df)) * 100
            st.metric("üìã Taxa de Arquivamento", f"{taxa_arquivamento:.1f}%")
        
        with col3:
            concluidos = len(anomalies_df[anomalies_df['status_investigacao'].str.contains('Conclu√≠do', na=False)])
            taxa_conclusao = (concluidos / len(anomalies_df)) * 100
            st.metric("‚úÖ Taxa de Conclus√£o", f"{taxa_conclusao:.1f}%")
        
        st.markdown("---")
        
        # Top crimes at√≠picos
        st.subheader("üìã Top 10 Crimes Mais At√≠picos")
        crime_dist = anomalies_df['tipo_crime'].value_counts()
        top_crimes = crime_dist.head(10)
        
        # Criar cards simples para cada crime
        cols = st.columns(2)
        for i, (crime, count) in enumerate(top_crimes.items()):
            percentage = (count / len(anomalies_df)) * 100
            col_idx = i % 2
            
            with cols[col_idx]:
                # Emoji baseado na posi√ß√£o
                if i == 0:
                    emoji = "ü•á"
                elif i == 1:
                    emoji = "ü•à"
                elif i == 2:
                    emoji = "ü•â"
                else:
                    emoji = "üî∏"
                
                st.info(f"{emoji} **{crime}**: {count} casos ({percentage:.1f}%)")
        
        st.markdown("---")
        
        # Distribui√ß√£o por status
        st.subheader("üìà Como Terminaram Estes Casos Especiais?")
        
        col1, col2 = st.columns(2)
        
        with col1:
            arquivados = len(anomalies_df[anomalies_df['status_investigacao'].str.contains('Arquivado', na=False)])
            perc_arquivados = (arquivados / len(anomalies_df)) * 100
            st.metric("üìã Casos Arquivados", arquivados, f"{perc_arquivados:.1f}% dos at√≠picos")
        
        with col2:
            concluidos = len(anomalies_df[anomalies_df['status_investigacao'].str.contains('Conclu√≠do', na=False)])
            perc_concluidos = (concluidos / len(anomalies_df)) * 100
            st.metric("‚úÖ Casos Conclu√≠dos", concluidos, f"{perc_concluidos:.1f}% dos at√≠picos")
    else:
        st.warning("‚ö†Ô∏è N√£o foi poss√≠vel mostrar os detalhes dos casos at√≠picos.")
else:
    st.info("""
    ‚ÑπÔ∏è **Nenhum caso at√≠pico encontrado com os filtros selecionados.**
    
    Isso pode significar que:
    - Todos os casos seguem padr√µes normais
    - Os filtros est√£o muito restritivos
    - Tente ajustar os filtros acima para ver mais casos
    """)

# Mapa de Hotspots (por √∫ltimo)
st.markdown("---")
st.header("üó∫Ô∏è Mapa de Hotspots")

st.markdown("""
**O mapa mostra onde os crimes acontecem com mais frequ√™ncia na cidade.**

**üîç O que √© um hotspot?**
- **Hotspot** = √°rea com alta concentra√ß√£o de crimes
- **C√≠rculos vermelhos** = √°reas com muitos crimes
- **C√≠rculos menores** = √°reas com poucos crimes
- **Cores mais escuras** = maior concentra√ß√£o de crimes

**üìä Como usar o mapa:**
- **Clique nos c√≠rculos** para ver detalhes de cada √°rea
- **Use o zoom** para explorar bairros espec√≠ficos
- **Compare as √°reas** para identificar padr√µes geogr√°ficos
- **Identifique zonas cr√≠ticas** que precisam de mais aten√ß√£o policial

**üéØ Por que isso √© importante?**
- Ajuda a **planejar patrulhamento** policial
- Identifica **√°reas de risco** para a popula√ß√£o
- Permite **aloca√ß√£o de recursos** de forma mais eficiente
- Facilita a **an√°lise de padr√µes** geogr√°ficos dos crimes
""")

# Fun√ß√£o para criar mapa de hotspots
def create_hotspot_map(df):
    """Cria mapa de hotspots usando Folium"""
    # Coordenadas reais de Recife
    center_lat, center_lon = -8.0476, -34.8770  # Recife como refer√™ncia
    
    # Criar mapa base
    m = folium.Map(location=[center_lat, center_lon], zoom_start=11)
    
    # Usar coordenadas reais dos bairros de Recife
    bairro_coords = {}
    
    # Coordenadas reais dos bairros de Recife (baseado nos bairros do dataset)
    recife_bairros = {
        'Imbiribeira': [-8.1114, -34.9435],  # Zona Sul
        'Boa Viagem': [-8.1000, -34.8833],   # Zona Sul, pr√≥ximo √† praia
        'Santo Amaro': [-8.0969, -34.8984], # Zona Sul
        'Afogados': [-8.1351, -34.9135],    # Zona Sul
        'Tamarineira': [-8.0333, -34.8833], # Zona Norte
        'Torre': [-8.0500, -34.9000],       # Centro
        'Casa Forte': [-8.0331, -34.9181],  # Zona Norte
        'Gra√ßas': [-8.0500, -34.9000],      # Centro
        'Espinheiro': [-8.0500, -34.8833],  # Centro
        'Pina': [-8.1000, -34.8833]        # Zona Sul
    }
    
    # Mapear cada bairro do dataset para suas coordenadas reais
    for bairro in df['bairro'].unique():
        if bairro in recife_bairros:
            bairro_coords[bairro] = recife_bairros[bairro]
        else:
            # Para bairros n√£o mapeados, usar coordenadas pr√≥ximas ao centro de Recife
            lat_offset = np.random.uniform(-0.03, 0.03)
            lon_offset = np.random.uniform(-0.03, 0.03)
            bairro_coords[bairro] = [center_lat + lat_offset, center_lon + lon_offset]
    
    # Contar crimes por bairro
    crime_counts = df['bairro'].value_counts()
    
    # Normalizar contagens para tamanho dos c√≠rculos (0.1 a 0.8)
    max_count = crime_counts.max()
    min_count = crime_counts.min()
    
    if max_count > min_count:
        normalized_sizes = 0.1 + 0.7 * (crime_counts - min_count) / (max_count - min_count)
    else:
        normalized_sizes = [0.4] * len(crime_counts)
    
    # Adicionar c√≠rculos para cada bairro
    for bairro, count in crime_counts.items():
        if bairro in bairro_coords:
            lat, lon = bairro_coords[bairro]
            size = normalized_sizes[bairro]
            
            # Cor baseada na quantidade de crimes
            if count >= max_count * 0.8:
                color = 'red'
            elif count >= max_count * 0.6:
                color = 'orange'
            elif count >= max_count * 0.4:
                color = 'yellow'
            else:
                color = 'green'
            
            # Adicionar c√≠rculo
            folium.CircleMarker(
                location=[lat, lon],
                radius=size * 50,  # Escalar para tamanho vis√≠vel
                popup=f"<b>{bairro}</b><br>Crimes: {count}",
                color='black',
                weight=1,
                fillColor=color,
                fillOpacity=0.6
            ).add_to(m)
    
    return m

# Criar e exibir mapa
if 'bairro' in df_filtered.columns:
    with st.spinner("üó∫Ô∏è Criando mapa de hotspots..."):
        hotspot_map = create_hotspot_map(df_filtered)
        st.components.v1.html(hotspot_map._repr_html_(), height=600)
        
        # Estat√≠sticas do mapa
        col1, col2, col3 = st.columns(3)
        with col1:
            total_bairros = df_filtered['bairro'].nunique()
            st.metric("Bairros no Mapa", total_bairros)
        with col2:
            bairro_mais_crimes = df_filtered['bairro'].value_counts().index[0]
            crimes_mais_bairro = df_filtered['bairro'].value_counts().iloc[0]
            st.metric("Bairro com Mais Crimes", f"{bairro_mais_crimes} ({crimes_mais_bairro} crimes)")
        with col3:
            total_crimes = len(df_filtered)
            st.metric("Total de Crimes no Mapa", total_crimes)
else:
    st.warning("‚ö†Ô∏è Coluna 'bairro' n√£o encontrada no dataset. Mapa de hotspots n√£o pode ser gerado.")

# Sidebar para informa√ß√µes
st.sidebar.header("üìä Informa√ß√µes do Sistema")

st.sidebar.metric("Total de Crimes Analisados", len(df_filtered))
st.sidebar.metric("Crimes Resolvidos", len(df_filtered[df_filtered['status_binario'] == 1]))
st.sidebar.metric("Crimes Arquivados", len(df_filtered[df_filtered['status_binario'] == 0]))

# Guia explicativo
with st.sidebar.expander("üìö Como Entender os Resultados", expanded=True):
    st.markdown("""
    **üéØ Probabilidades:**
    - **Alta (>70%)**: Muito prov√°vel que aconte√ßa
    - **M√©dia (30-70%)**: Incerto, pode acontecer ou n√£o
    - **Baixa (<30%)**: Pouco prov√°vel que aconte√ßa
    
    **üë• Grupos de Crimes:**
    - Crimes similares s√£o agrupados juntos
    - Cada grupo tem caracter√≠sticas parecidas
    - Ajuda a entender padr√µes
    
    **üö® Casos At√≠picos:**
    - Crimes muito diferentes do normal
    - Merecem aten√ß√£o especial
    - Podem indicar novos tipos de crime
    
    **üîß Ajuste Autom√°tico:**
    - Melhora a precis√£o do sistema
    - Testa diferentes configura√ß√µes
    - Pode demorar alguns minutos
    """)

# An√°lise explorat√≥ria
st.header("üìà Vis√£o Geral dos Dados")

st.markdown("**Aqui voc√™ pode ver como os crimes est√£o distribu√≠dos no sistema:**")

col1, col2 = st.columns(2)

with col1:
    # Distribui√ß√£o do status
    status_counts = df_filtered['status_investigacao'].value_counts()
    st.write("**üìä Como terminam os crimes no sistema:**")
    for status, count in status_counts.items():
        percentage = (count / len(df_filtered)) * 100
        st.write(f"‚Ä¢ {status}: {count} casos ({percentage:.1f}% do total)")

with col2:
    # Status por tipo de crime
    status_crime = pd.crosstab(df_filtered['tipo_crime'], df_filtered['status_investigacao'])
    st.write("**üìà Como cada tipo de crime costuma terminar:**")
    st.dataframe(status_crime, width='stretch')

# M√©tricas do modelo
st.header("üìä Qualidade do Sistema")

accuracy = accuracy_score(y_test, y_pred)

col1, col2, col3 = st.columns(3)
with col1:
    st.metric("Precis√£o do Sistema", f"{accuracy:.1%}")
    st.caption("Quantos casos o sistema acerta")
with col2:
    # Calcular precis√£o m√©dia
    precision = precision_score(y_test, y_pred, average='weighted')
    st.metric("Confiabilidade", f"{precision:.1%}")
    st.caption("Qu√£o confi√°vel √© o sistema")
with col3:
    st.metric("Casos Testados", len(y_test))
    st.caption("Quantos casos foram usados para testar")

# Configura√ß√µes do sistema (simplificado)
st.subheader("‚öôÔ∏è Configura√ß√µes do Sistema")
st.info("O sistema usa Random Forest com configura√ß√µes otimizadas para an√°lise de crimes.")




